---
output: 
  pdf_document: 
    fig_caption: true
    fig_width: 6
    fig_height: 4

params:
  date: !r Sys.Date()
  # target.label: "PERF.all"
  target.label: "LIFE_S_R"
  # features.set: "big5items"
  features.set: "big5composites"
  split.ratio: 0.80
  cv.repeats: 100
  # cv.repeats: 10
  impute.method: "noimpute"
  # impute.method: "medianImpute"
  # impute.method: "bagImpute"

title: "Job Performance Analysis with
        target = `r params$target.label` and
        features set = `r params$features.set`"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# knitr::opts_knit$set(global.par = TRUE)
options(digits = 3)

# clear the workspace
# rm(list=ls()) # tricky: deletes params

# devtools::install_github("agilebean/machinelearningtools", force = TRUE)
# detach("package:machinelearningtools", character.only = TRUE)
# load libraries
libraries <- c("dplyr", "magrittr"
               , "sjlabelled" # read SPSS
               , "caret", "doParallel"
               , "DataExplorer", "RColorBrewer"
               , "machinelearningtools"
               , "knitr", "pander"
               , "tidyverse"
)
sapply(libraries, require, character.only = TRUE)

if (params$impute.method != "noimpute") {
  dataset.label <- "data/dataset.rds" %>% print
} else {
  dataset.label <- "data/dataset.NA.rds" %>% print
}

# nominal <- FALSE # with ordinal as ORDERED factors
nominal <- TRUE # with ordinal as NOMINAL factor

seed <- 17

WIDTH.varimp <- if (params$features.set == "big5items") 8 else 4
HEIGHT.varimp <- if (params$features.set == "big5items") 8 else 2

get_features <- function(target_label, features_set, data_set) {
  
  data_set %>% 
    select(-target_label,
           -starts_with("TO"),
           -starts_with("PERF"),
           -LIFE_S_R
    ) %>% 
    {
      if (features_set == "big5items") {
        # remove composite scores - equivalent to (-nn, -ee, -oo, -aa, -cc)
        select(., -matches("(oo|cc|ee|aa|nn)$")) 
        
      } else if (features_set == "big5composites") {
        # remove Big5 items
        select(., -matches(".*(1|2|3|4|5|6)"))
        
      } else { . }
    } %>% 
    names %T>% print
}
```


# 1. Data Acquistion

```{r data acquisition, include=TRUE}
#######################################################################
# 1. Data Acquistion
#######################################################################
dataset.label %>% print
dataset <- readRDS(dataset.label)

```

# 2. Data Preparation

## 2.4 Select the target & features

```{r select target & features code, include=FALSE}
########################################
## 2.4 Select the target & features
########################################

set.seed(seed)
dataset %<>% nrow %>% sample %>% dataset[.,]

# rerun big5 without covariates
# dataset %<>% select(-COMNAME, -educa, -gender, -LIFE_S_R, -inf, -sd)

########################################
# 3.2: Select the features & formula
########################################
# define features
features.labels <- get_features(params$target.label, params$features.set, dataset)

# define formula
formula1 <- set_formula(params$target.label, features.labels)

```

For this analysis, the target is set to 
`r if (params$target.label == "PERF.all") { paste("the averaged performance score")} else if (grepl("PERF[0-9]", params$target.label)) { paste("a one-year performance score")} else if (params$target.label == "TO.all") { paste("the averaged turnover score")} else if (params$target.label == "LIFE_S_R") { paste("the life satisfaction score")} else {paste(params$target.label)}`, 
and the features are 
`r if (params$features.set == "big5items") {paste("all Big5 items")} else if (params$features.set =="big5composites") {paste("the Big5 composite scores")} else {paste(params$features.set)}`.

```{r select target & features, echo=TRUE}
params$target.label
features.labels
```

## 2.3 Split the data
To train the machine learning models, we split the data into a training set (`r params$split.ratio*100`%) for training and tuning the models, and a testing set (`r 100-params$split.ratio*100`%) that serves as unseen data for the final validation step.


```{r split the data code, include=FALSE}
########################################
## 2.3 Split the data
########################################

## tricky tricky:
## shuffle reduces testing set RMSE https://stackoverflow.com/q/58147450/7769076
# shuffle data
# set.seed(seed)
# dataset %<>% nrow %>% sample %>% dataset[.,] %T>% print

# later: imputation of NAs
# dataset %>% preProcess(method="knnImpute") %>% print
if (params$impute.method != "noimpute") {
  
  dataset.imputed <- dataset %>% 
    preProcess(method = params$impute.method) %>% 
    predict(model.imputed, newdata = dataset) %T>% print
  
  dataset <- dataset.imputed %>% na.omit
}

# select variables - for different targets, #NA can differ
dataset <- dataset %>%
  select(params$target.label, features.labels) %>%
  na.omit

# dataset subsetting for tibble: [[
training.index <- createDataPartition(dataset[[params$target.label]], 
                                      p = params$split.ratio, list = FALSE)
testing.set <- dataset[-training.index, ]
training.set <- dataset[training.index, ]

```

We verify the `r params$split.ratio*100`:`r 100-params$split.ratio*100` split by checking the number of observations:

```{r split the data}
training.set %>% nrow
testing.set %>% nrow
```

# 3. Train Model

```{r train the model code, include=FALSE}
################################################################################
# 3. Train Model
# 3-1: Select a model
# 3-2: Select the target, features, training data
# 3-3: Train the model with the target and features
################################################################################

########################################
# 3-1: Select a model
########################################
algorithm.list <- c(
  "lm",
  "knn",
  "gbm",
  "rf",
  "ranger",
  "xgbTree",
  "XgbLinear",
  "svmLinear",
  "svmRadial"
)

```

The machine learning models used in the benchmarking are:

**Linear models:**
  | "lm": linear regression (benchmarking baseline)
  | "glm": logistic regression
  | "xgbLinear": extreme gradient boosting  - linear kernel
  | "svmLinear": support vector machines - linear kernel
  
**Tree-based models:**
  | "gbm": gradient boosting machines
  | "rf", "ranger": random forests
  | "xgbTree": extreme gradient boosting - tree kernel
  
**Other non-linear models:**
  | "knn": k nearest neighbors
  | "svmRadial": support vector machines - radial kernel

\newpage

# 4. Evaluate Models
```{r get model, cache=TRUE, include=FALSE}
models.list.name <- paste0(c("data/models.list", 
                          params$target.label, 
                          params$features.set,
                          paste0(params$cv.repeats, "repeats"),
                          params$impute.method,
                          "rds"), 
                          collapse = ".") %T>% print

models.list <- models.list.name %>% readRDS

training_configuration_number <- models.list[[1]] %>% .$control %>% .$number
training_configuration_repeats <- models.list[[1]] %>% .$control %>% .$repeats

```

Training the model on the training set with `r training_configuration_number`-fold cross-validation, repeated  `r training_configuration_repeats` times, yields the following results. Linear regression as benchmark reference is denoted as "lm".

## 4.1 Training Set Performance

```{r trainingset calculation, cache=TRUE, echo=FALSE}

# get model metrics
models.metrics <- models.list %>% get_model_metrics
models.list %<>% purrr::list_modify(target.label = NULL, testing.set = NULL)

# models.metrics$RMSE.training %>% filter(str_detect(model, "^lm$"))
metric1 <- models.metrics$metric1.training %>% select(2) %>% names %>% gsub(".mean", "", .)
metric2 <- models.metrics$metric1.training %>% select(2) %>% names %>% gsub(".mean", "", .)
rank.lm.training.metric1 <- which(models.metrics$metric1.training$model == "lm")

# get_metric_from_resamples
# models.metrics$metric1.training %>% arrange(RMSE.mean)

# display RMSE trainingset performance - table
models.metrics$metric1.training %>% kable(caption = "training set performance: RMSE")
```

In the benchmarking table above, linear regression ("lm") performs in `r metric1` on `r rank.lm.training.metric1`th rank of the selected machine learning models.


```{r trainingset metric1 boxplots,fig.width=7, fig.height=4.8, cache=TRUE, echo=FALSE}
# display RMSE trainingset performance - boxplots
models.metrics$metric1.resamples.boxplots
# models.metrics$metric1.resamples.boxplots + 
#   theme(text = element_text(family = 'Gill Sans'))
```
\newpage

```{r trainingset metric2 , cache=FALSE}
# display Rsquared trainingset performance - table
models.metrics$metric2.training %>% kable(caption = "training set performance: Rsquared")

# models.metrics$metric2.resamples.boxplots +
#   theme(text = element_text(family = 'Gill Sans'))
rank.lm.training.metric2 <- which(models.metrics$metric2.training$model == "lm")
```

In the benchmarking table above, linear regression ("lm") performs in `r metric2` on `r rank.lm.training.metric2`th  rank of the selected machine learning models.

```{r trainingset metric2 boxplots,fig.width=7, fig.height=5, cache=TRUE}
# display Rsquared trainingset performance - boxplots
models.metrics$metric2.resamples.boxplots

```
\newpage

## 4.2 Testing Set Performance

The Rsquared for testing set was calculated with three formulas:

* "R2.testing" calculates the performance relative to the trainingset mean.
* "R2.testing1" calculates the performance relative to the testingset mean.
* "R2.postResample" uses the (incorrect) caret implementation for Rsquared by the correlation between observed and predicted values.

```{r testing set RMSE, cache=FALSE}
########################################
## 4.2 Testing Set Performance
########################################

## very tricky: models.list is overridden by previous code
## solution: read again
# models.list <- models.list.name %>% readRDS
# 
# # RMSE for all models on testing set
# models.list %>%
#   get_model_metrics %>%
#   .$RMSE.testing %>% kable(caption = "testing set performance: RMSE")
# 
# # training vs. testing set performance
# models.list %>%
#   get_model_metrics %>%
#   .$RMSE.all %>% kable(caption = "training vs. testing set performance: RMSE")

# models.metrics$metrics.testing
models.metrics$metrics.testing %>% kable(caption = "testing set performance: RMSE")

rank.lm.testing <-  which(models.metrics$metrics.testing$model == "lm")


```

We had reserved a portion of the original data as testing set to validate the model performance on unseen data. 
The model comparison on `r metric1` shows that linear regression model `r if (rank.lm.testing == rank.lm.training.metric1) {paste0("remained on ", rank.lm.training.metric1, "th rank.")} else if (rank.lm.testing < rank.lm.training.metric1) {paste0("improved from ", rank.lm.training.metric1, "th to ", rank.lm.testing, "th rank.")} else {paste0("dropped from ", rank.lm.training.metric1, "th to ", rank.lm.testing, "th rank.")}`

The following table compares the training set performance with the  testing set performance for all models.

```{r testing set benchmark all}
models.metrics$benchmark.all %>% 
  select(-R2.testing2, -R2.postResample) %>% 
  kable(caption = "training vs. testing set performance: RMSE")
```

\newpage
## 4.4 Variable Importance
The variable importance can be calculated for linear regression, gradient boosting machines and random forests only. The scores are normalized with 100 for the maximum variable importance among all predictors. 

``` {r variable importance, fig.width=WIDTH.varimp, fig.height=HEIGHT.varimp, cache=TRUE}
models.varimp <- models.list %>% head(-5) %>% list_modify(knn = NULL)

visualize_importance <- function (importance_object) {

  if (class(importance_object) == "varImp.train") {
    importance_object %<>% .$importance
  }
  if (!hasName(importance_object, "rowname")) {
    importance_object %<>% rownames_to_column()
  }

  importance_object %>%
    setNames(c("variable", "Importance")) %>%
    ggplot(data = ., aes(x = reorder(variable, Importance), y = Importance)) +
    theme_minimal() +
    geom_bar(stat = "identity", fill = "#114151") +
    coord_flip() +
    theme(axis.title = element_text(size = 12),
          axis.text = element_text(size = 12)) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 102)) +
    xlab("item") + ylab("variable importance")
}

library(gbm)
tables.varimp <- models.varimp %>% map(~varImp(.))

varimp.plots <- models.varimp %>%
  map(function(model) {
    require(gbm)
    model %>% varImp %>% visualize_importance()
  })

tables.varimp$lm
varimp.plots$lm

tables.varimp$glm
varimp.plots$glm

tables.varimp$gbm
varimp.plots$gbm

tables.varimp$rf
varimp.plots$rf

```


