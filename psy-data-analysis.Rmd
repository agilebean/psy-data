---
output: 
  pdf_document: 
    fig_caption: true
    fig_width: 6
    fig_height: 4
  keep_tex: yes

params:
  date: !r Sys.Date()
  # target.label: "PERF.all"
  target.label: "PERF10"
  # features.set: "big5items"
  features.set: "big5composites"
  job.label: "sales"
  split.ratio: 1.0
  cv.repeats: 100
  # cv.repeats: 10
  impute.method: "noimpute"
  # impute.method: "medianImpute"
  # impute.method: "bagImpute"

title: "Job Performance Analysis for 
        job type = `r params$job.label` with
        target = `r params$target.label` and
        features set = `r params$features.set`"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# knitr::opts_knit$set(global.par = TRUE)
options(digits = 3)

# clear the workspace
# rm(list=ls()) # tricky: deletes params

# devtools::install_github("agilebean/machinelearningtools", force = TRUE)
# detach("package:machinelearningtools", character.only = TRUE)
# load libraries
libraries <- c("dplyr", "magrittr"
               , "sjlabelled" # read SPSS
               , "caret", "doParallel"
               , "DataExplorer", "RColorBrewer"
               , "machinelearningtools"
               , "knitr", "pander"
               , "tidyverse"
)
sapply(libraries, require, character.only = TRUE)

if (params$impute.method != "noimpute") {
  dataset.label <- "data/dataset.rds" %>% print
} else {
  dataset.label <- "data/dataset.NA.rds" %>% print
}

# nominal <- FALSE # with ordinal as ORDERED factors
nominal <- TRUE # with ordinal as NOMINAL factor

seed <- 17

WIDTH.varimp <- if (params$features.set == "big5items") 8 else 4
HEIGHT.varimp <- if (params$features.set == "big5items") 8 else 2

get_features <- function(data, target_label, features_set_label) {

  data %>%
    select(-(target_label),
           -starts_with("TO"),
           -starts_with("PERF"),
           -starts_with("LIFE"),
           -job
    ) %>%
    {
      if (features_set_label == "big5items") {
        # remove composite scores - equivalent to (-nn, -ee, -oo, -aa, -cc)
        select(., -matches("(oo|cc|ee|aa|nn)$"))

      } else if (features_set_label == "big5composites") {
        # remove Big5 items
        select(., -matches(".*(1|2|3|4|5|6)"))

      } else { . }
    } %>%
    names
}
```


# 1. Data Acquistion

```{r data acquisition, include=TRUE}
#######################################################################
# 1. Data Acquistion
#######################################################################
dataset.label %>% print
dataset <- readRDS(dataset.label)

```

# 2. Data Preparation

## 2.4 Select the target & features

```{r select target & features code, include=FALSE}
########################################
## 2.4 Select the target & features
########################################

set.seed(seed)
dataset %<>% nrow %>% sample %>% dataset[.,]

# rerun big5 without covariates
# dataset %<>% select(-COMNAME, -educa, -gender, -LIFE_S_R, -inf, -sd)

########################################
# 3.2: Select the features & formula
########################################
# define features
features.labels <- get_features(dataset, params$target.label, params$features.set)

```

For this analysis, the job type is
`r params$job.label`, the target is set to 
`r if (params$target.label == "PERF.all") { paste("the averaged performance score")} else if (grepl("PERF[0-9]", params$target.label)) { paste("a one-year performance score")} else if (params$target.label == "TO.all") { paste("the averaged turnover score")} else if (params$target.label == "LIFE_S_R") { paste("the life satisfaction score")} else {paste(params$target.label)}`, 
and the features are 
`r if (params$features.set == "big5items") {paste("all Big5 items")} else if (params$features.set =="big5composites") {paste("the Big5 composite scores")} else {paste(params$features.set)}`.

```{r select target & features, echo=TRUE}
params$target.label
features.labels

```


# 3. Train Model

```{r train the model code, include=FALSE}
################################################################################
# 3. Train Model
# 3-1: Select a model
# 3-2: Select the target, features, training data
# 3-3: Train the model with the target and features
################################################################################

########################################
# 3-1: Select a model
########################################
algorithm.list <- c(
  "lm",
  "knn",
  "gbm",
  "rf",
  "ranger",
  "xgbTree",
  "XgbLinear",
  "svmLinear",
  "svmRadial"
)

```

The machine learning models used in the benchmarking are:

**Linear models:**
  | "lm": linear regression (benchmarking baseline)
  | "glmnet": linear regression - gaussian kernel
  | "xgbLinear": extreme gradient boosting  - linear kernel
  | "svmLinear": support vector machines - linear kernel
  
**Tree-based models:**
  | "gbm": gradient boosting machines
  | "xgbTree": extreme gradient boosting - tree kernel
  | "rf", "ranger": random forests
  
**Other non-linear models:**
  | "knn", "kknn: k nearest neighbors
  | "svmRadial": support vector machines - radial kernel

\newpage

# 4. Evaluate Models
```{r get model, cache=TRUE, include=FALSE}

models.list.name <- output_filename(
  "data/models.list",
  c(params$target.label, params$features.set, params$job.label),
  cv_repeats = params$cv.repeats, impute_method = params$impute.method,
  suffix = "rds"
)

models.list <- models.list.name %>% readRDS

training_configuration_number <- models.list[[1]] %>% .$control %>% .$number
training_configuration_repeats <- models.list[[1]] %>% .$control %>% .$repeats
training_data_dimension <- models.list[[1]]$trainingData %>% dim
```

Training the model on the training set that contained `r training_data_dimension[1]` observations with `r training_configuration_number`-fold cross-validation, repeated  `r training_configuration_repeats` times, yields the following results. Linear regression as benchmark reference is denoted as "lm".

## 4.1 Training Set Performance

```{r trainingset calculation, cache=TRUE, echo=FALSE}

# get model metrics
models.metrics <- models.list %>% get_model_metrics
models.list %<>% purrr::list_modify(target.label = NULL, testing.set = NULL)

# models.metrics$RMSE.training %>% filter(str_detect(model, "^lm$"))
metric1 <- models.metrics$metric1.training %>% select(2) %>% names %>% gsub(".mean", "", .)
metric2 <- models.metrics$metric1.training %>% select(2) %>% names %>% gsub(".mean", "", .)
rank.lm.training.metric1 <- which(models.metrics$metric1.training$model == "lm")

# get_metric_from_resamples
# models.metrics$metric1.training %>% arrange(RMSE.mean)

# display RMSE trainingset performance - table
models.metrics$metric1.training %>% kable(caption = "training set performance: RMSE")

```

In the benchmarking table above, linear regression ("lm") performs in `r metric1` on `r rank.lm.training.metric1`th rank of the selected machine learning models.


```{r trainingset metric1 boxplots,fig.width=7, fig.height=4.8, cache=TRUE, echo=FALSE}
# display RMSE trainingset performance - boxplots
models.metrics$metric1.resamples.boxplots
# models.metrics$metric1.resamples.boxplots + 
#   theme(text = element_text(family = 'Gill Sans'))
```
\newpage

```{r trainingset metric2 , cache=FALSE}
# display Rsquared trainingset performance - table
models.metrics$metric2.training %>% kable(caption = "training set performance: Rsquared")

# models.metrics$metric2.resamples.boxplots +
#   theme(text = element_text(family = 'Gill Sans'))
rank.lm.training.metric2 <- which(models.metrics$metric2.training$model == "lm")
```

In the benchmarking table above, linear regression ("lm") performs in `r metric2` on `r rank.lm.training.metric2`th  rank of the selected machine learning models.

```{r trainingset metric2 boxplots,fig.width=7, fig.height=5, cache=TRUE}
# display Rsquared trainingset performance - boxplots
models.metrics$metric2.resamples.boxplots

```

\newpage
## 4.4 Variable Importance
The variable importance can be calculated for linear regression, gradient boosting machines and random forests only. The scores are normalized with 100 for the maximum variable importance among all predictors. 

``` {r variable importance, fig.width=WIDTH.varimp, fig.height=HEIGHT.varimp, cache=TRUE}
models.varimp <- models.list %>% 
  names %>% 
  str_detect("lm|glmnet|gbm|rf") %>%
  purrr::keep(models.list, .)

visualize_importance <- function (importance_object) {

  if (class(importance_object) == "varImp.train") {
    importance_object %<>% .$importance
  }
  if (!hasName(importance_object, "rowname")) {
    importance_object %<>% rownames_to_column()
  }

  importance_object %>%
    setNames(c("variable", "Importance")) %>%
    ggplot(data = ., aes(x = reorder(variable, Importance), y = Importance)) +
    theme_minimal() +
    geom_bar(stat = "identity", fill = "#114151") +
    coord_flip() +
    theme(axis.title = element_text(size = 12),
          axis.text = element_text(size = 12)) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, 102)) +
    xlab("item") + ylab("variable importance")
}

library(gbm)
tables.varimp <- models.varimp %>% map(~varImp(.))

varimp.plots <- models.varimp %>%
  map(function(model) {
    require(gbm)
    model %>% varImp %>% visualize_importance()
  })

tables.varimp$lm
varimp.plots$lm

tables.varimp$glm
varimp.plots$glm

tables.varimp$gbm
varimp.plots$gbm

tables.varimp$rf
varimp.plots$rf

```


